{
    "BestFirstSearch": {
      "code": "import heapq\n\ngraph = {\n    'A': ['B', 'C'],\n    'B': ['D', 'E'],\n    'C': ['F'],\n    'D': [],\n    'E': [],\n    'F': []\n}\n\nheuristics = {\n    'A': 5,\n    'B': 4,\n    'C': 3,\n    'D': 2,\n    'E': 6,\n    'F': 1 \n}\n\ndef best_first_search(start, goal):\n    visited = set()\n    queue = []\n    heapq.heappush(queue, (heuristics[start], start))\n    path = []\n    while queue:\n        h, current = heapq.heappop(queue)\n        if current in visited:\n            continue\n        visited.add(current)\n        path.append(current)\n        print(f\"Visiting: {current} (h={h})\")\n        if current == goal:\n            print(\"Goal reached!\")\n            return path\n        for neighbor in graph[current]:\n            if neighbor not in visited:\n                heapq.heappush(queue, (heuristics[neighbor], neighbor))\n    return path\n\nstart_node = 'A'\ngoal_node = 'F'\nresult_path = best_first_search(start_node, goal_node)\nprint(\"\\nüõ£Ô∏è Path taken:\", \" ‚Üí \".join(result_path))"
    },
    "AOStar": {
      "code": "def ao_star_with_g(node, graph, heuristics, solved, solution, g_values, g=0):\n    print(f\"Expanding Node: {node} | h(n) = {heuristics[node]}, g(n) = {g}, f(n) = {g + heuristics[node]}\")\n    g_values[node] = g\n    if node in solved:\n        return heuristics[node]\n    if not graph[node]:\n        solved[node] = True\n        return heuristics[node]\n    min_cost = float('inf')\n    best_successors = None\n    for successor in graph[node]:\n        if isinstance(successor, tuple) and successor[1] == 'AND':\n            children = successor[0]\n            total_h = sum(heuristics[child] for child in children)\n            cost = g + total_h\n            if cost < min_cost:\n                min_cost = cost\n                best_successors = successor\n        else:\n            child = successor[0] if isinstance(successor, tuple) else successor\n            cost = g + heuristics[child]\n            if cost < min_cost:\n                min_cost = cost\n                best_successors = successor\n    heuristics[node] = min_cost - g\n    solution[node] = best_successors\n    if isinstance(best_successors, tuple) and best_successors[1] == 'AND':\n        all_solved = True\n        for child in best_successors[0]:\n            ao_star_with_g(child, graph, heuristics, solved, solution, g_values, g + heuristics[child])\n            all_solved &= solved.get(child, False)\n        if all_solved:\n            solved[node] = True\n    else:\n        child = best_successors[0] if isinstance(best_successors, tuple) else best_successors\n        ao_star_with_g(child, graph, heuristics, solved, solution, g_values, g + heuristics[child])\n        if solved.get(child, False):\n            solved[node] = True\n    return heuristics[node]"
    },
    "AStar": {
      "code": "import heapq\n\ndef astar(graph, heuristics, start, goal):\n    open_set = []\n    heapq.heappush(open_set, (heuristics[start], 0, start, [start]))\n    while open_set:\n        f, g, current, path = heapq.heappop(open_set)\n        print(f\"Visiting: {current}  , h(n) = {heuristics[current]}, g(n) = {g}, f(n) = {f}\")\n        if current == goal:\n            return path, g\n        for neighbor, weight in graph[current]:\n            new_g = g + weight\n            new_f = new_g + heuristics[neighbor]\n            heapq.heappush(open_set, (new_f, new_g, neighbor, path + [neighbor]))\n    return None, float('inf')\n\ngraph = {\n    'A': [('B', 1), ('C', 3)],\n    'B': [('D', 1), ('E', 4)],\n    'C': [('F', 2)],\n    'D': [],\n    'E': [],\n    'F': []\n}\n\nheuristics = {\n    'A': 5,\n    'B': 3,\n    'C': 4,\n    'D': 2,\n    'E': 6,\n    'F': 0\n}\n\nstart_node = 'A'\ngoal_node = 'F'\npath, cost = astar(graph, heuristics, start_node, goal_node)\nif path:\n    print(\"\\n‚úÖ Final Path:\", \" -> \".join(path))\n    print(\"üßÆ Total Cost:\", cost)"
    },
    "GeneticAlgorithm": {
      "code": "import random\n\ndef fitness(x):\n    return x**2 - 3*x + 2\n\ndef mutate(x):\n    return x + random.uniform(-0.1, 0.1)\n\ndef crossover(x, y):\n    return (x + y) / 2\n\npopulation = [random.uniform(-10, 10) for _ in range(10)]\nfor generation in range(100):\n    population = sorted(population, key=lambda x: fitness(x))\n    new_population = population[:5]\n    while len(new_population) < 10:\n        parent1 = random.choice(new_population)\n        parent2 = random.choice(new_population)\n        child = crossover(parent1, parent2)\n        if random.random() < 0.1:\n            child = mutate(child)\n        new_population.append(child)\n    population = new_population\n\nbest_individual = min(population, key=lambda x: fitness(x))\nprint(f\"Best individual: {best_individual}, Fitness: {fitness(best_individual)}\")"
    },
    "FuzzyLogic": {
      "code": "def triangular_membership(x, a, b, c):\n    if x <= a or x >= c:\n        return 0\n    elif x == b:\n        return 1\n    elif a < x < b:\n        return (x - a) / (b - a)\n    elif b < x < c:\n        return (c - x) / (c - b)\n    return 0\n\ndef fuzzify(x):\n    return {\n        'low': triangular_membership(x, 0, 0, 5),\n        'high': triangular_membership(x, 5, 10, 10)\n    }\n\ndef fuzzy_wash_time(dirt, grease):\n    dirt_fuzz = fuzzify(dirt)\n    grease_fuzz = fuzzify(grease)\n    short_strength = min(dirt_fuzz['low'], grease_fuzz['low'])\n    long_strength = max(dirt_fuzz['high'], grease_fuzz['high'])\n    total_strength = short_strength + long_strength\n    if total_strength == 0:\n        return 0\n    wash_time = (short_strength * 30 + long_strength * 90) / total_strength\n    return wash_time\n\ndirt = 7\ngrease = 8\ntime = fuzzy_wash_time(dirt, grease)\nprint(f\"Recommended wash time: {time:.2f} minutes\")"
    },
    "ANN": {
      "code": "from sklearn.neural_network import MLPClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\ndata = load_iris()\nX = data.data\ny = data.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nconfigs = [\n    {\"hidden_layer_sizes\": (5,), \"activation\": 'relu'},\n    {\"hidden_layer_sizes\": (10, 5), \"activation\": 'tanh'},\n    {\"hidden_layer_sizes\": (20, 10, 5), \"activation\": 'logistic'}\n]\n\nfor i, cfg in enumerate(configs):\n    print(f\"\\nüîπ Configuration {i+1}: Hidden Layers = {cfg['hidden_layer_sizes']}, Activation = {cfg['activation']}\")\n    model = MLPClassifier(hidden_layer_sizes=cfg['hidden_layer_sizes'], activation=cfg['activation'], max_iter=1000, random_state=1)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    print(classification_report(y_test, y_pred, target_names=data.target_names))"
    },
    "RandomForest": {
      "code": "from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\ndata = load_iris()\nX = data.data\ny = data.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\ndt_model = DecisionTreeClassifier(random_state=0)\ndt_model.fit(X_train, y_train)\ndt_pred = dt_model.predict(X_test)\ndt_acc = accuracy_score(y_test, dt_pred)\n\nrf_model = RandomForestClassifier(n_estimators=100, random_state=0)\nrf_model.fit(X_train, y_train)\nrf_pred = rf_model.predict(X_test)\nrf_acc = accuracy_score(y_test, rf_pred)\n\nprint(\"üå≥ Decision Tree Accuracy:\", dt_acc)\nprint(\"üå≤ Random Forest Accuracy:\", rf_acc)"
    }
  }
  